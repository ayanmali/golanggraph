/*
Go implementation of Claude logic from Anthropic API
*/

package main

import (
	"bytes"
	"encoding/json"
	"fmt"
	"io"
	"net/http"
	"os"
	"strings"
)

type LLM struct {
	// Available Models
	// - claude-3-7-sonnet-20250219
	// - claude-3-5-sonnet-20241022 (claude-3-5-sonnet-latest)
	// - claude-3-5-haiku-20241022 (claude-3-5-haiku-latest)
	// - claude-3-opus-20240229 (claude-3-opus-latest)
	// - claude-3-sonnet-20240229
	// - claude-3-haiku-20240307
	Model    string
	ApiKey   string
	System   []Content
	Messages []Message
	Tools    []Tool
}

/*
Creates a new client to interact with the LLM.
The apiKey argument is optional; omitting it will attempt to obtain the ANTHROPIC_API_KEY from the user's environment variables.
Note that if the ANTHROPIC_API_KEY environment variable is not set, the function will return `false` as its second return value.
If the apiKey argument is included, the LLM will use the provided API key for making requests to the API.
*/
func (llm LLM) NewClient(model string, apiKey ...string) (*LLM, bool) {
	if len(apiKey) == 0 {
		userApiKey, ok := os.LookupEnv("ANTHROPIC_API_KEY")
		if !ok {
			return nil, false
		}
		return &LLM{
			Model:  model,
			ApiKey: userApiKey,
		}, true
	}

	return &LLM{
		Model:    model,
		ApiKey:   apiKey[0],
		Messages: make([]Message, 0),
	}, true
}

type LLMRequest struct {
	Model string `json:"model"`
	// the maximum number of tokens for the LLM to generate
	MaxTokens int    `json:"max_tokens,omitempty"`
	Tools     []Tool `json:"tools,omitempty"`
	/*
		Specify how the LLM should use the given tools. 'any' forces a tool to be used, 'tool' forces a specific tool to be used, and 'auto' does not enforce any tool usage.
		Follows schema: {"type" : "auto/any/tool"} --- If "type" is set to "tool", then you must include another key "name", which is the name of the tool to force the LLM to use.
		Parallel tool use can be disabled by adding `"disable_parallel_tool_use" = true`.
	*/
	ToolChoice map[string]string `json:"tool_choice,omitempty"`
	/*
		Defines the conversation history with the LLM
	*/
	Messages []Message `json:"messages"`
	// System prompt for LLM
	System []Content `json:"system,omitempty"`
	// Temperature value (0 - 1)
	Temperature float32 `json:"temperature,omitempty"`
}

// Defines the structure of a single message (i.e. either system or user prompt)
type Message struct {
	Role string `json:"role"` // 'user' or 'assistant'
	/*
			The text to pass into the LLM.
			Example:
			messages=[
		        {
		            "role": "user",
		            "content": "What's the weather like in San Francisco?"
		        },
		        {
		            "role": "assistant",
		            "content": [
		                {
		                    "type": "text",
		                    "text": "<thinking>I need to use get_weather, and the user wants SF, which is likely San Francisco, CA.</thinking>"
		                },
		                {
		                    "type": "tool_use",
		                    "id": "toolu_01A09q90qw90lq917835lq9",
		                    "name": "get_weather",
		                    "input": {"location": "San Francisco, CA", "unit": "celsius"}
		                }
		            ]
		        }
			]
	*/
	Content []Content `json:"content"`
}

// type Source struct {
// 	// E.g. "base64"
// 	Type string `json:"type"`
// 	// E.g. "image/jpeg"
// 	MediaType string `json:"media_type"`
// 	// E.g. "/9j/4AAQSkZJRg..."
// 	Data string `json:"data"`
// }

type Content struct {
	/*
		The type of content being provided back to the user/LLM.
		The type can be 'text', 'tool_use' (used when providing a Tool to the LLM for it to use), or 'tool_result' (used when passing the output of a Tool call back to the LLM)
	*/
	Type string `json:"type"`
	Text string `json:"text,omitempty"` // LLM response itself
	/*
		An ID value generated by the LLM's response when tool use is enabled.
		Use this value when passing the result of a tool call back to the LLM in another message
	*/
	ID    string            `json:"id,omitempty"`
	Name  string            `json:"name,omitempty"`
	Input map[string]string `json:"input,omitempty"`
	/*
		The ID from the initial tool use call to the LLM.
		This ID is generated in the response after making an initial LLM call with tool use enabled.
		Use this key with 'role' = 'user' when passing the result from a tool call back to the LLM. Be sure to also include the 'content' key
	*/
	ToolUseID string `json:"tool_use_id,omitempty"`
	/*
		The output from the tool call.
		Use this key when passing the result from the tool call back to the LLM
	*/
	Content string `json:"content,omitempty"`
}

// type ResponseMessage struct {
// 	Content []Content `json:"content"`
// 	Role    string    `json:"role"`
// }

type LLMResponse struct {
	/*
		Contains multiple Content structs which correspond to different parts of the LLM's output.
		E.g. type "text" is the LLM's raw response, type "tool_use" is the model's tool call.
	*/
	Content []Content `json:"content"`
	ID      string    `json:"id"`
	Model   string    `json:"model,omitempty"`
	Role    string    `json:"role,omitempty"`
	//Type         string          `json:"type"`
	StopReason   string         `json:"stop_reason,omitempty"`
	StopSequence any            `json:"stop_sequence,omitempty"`
	Usage        map[string]int `json:"usage,omitempty"`
	//Message      ResponseMessage `json:"message"`
}

/*
Gets the LLM's response from the user's request
*/
func (response LLMResponse) getOutput() string {
	var sb strings.Builder
	for _, content := range response.Content {
		if content.Type == "text" {
			sb.WriteString(content.Text)
			sb.WriteString("\n\n")

		} else if content.Type == "tool_use" {
			sb.WriteString(fmt.Sprintf("Tool Use ID: %s\n", content.ID))
			sb.WriteString(fmt.Sprintf("Tool name: %s\n", content.Name))
			for k, v := range content.Input {
				sb.WriteString(fmt.Sprintf("Input Parameter \"%s\" = \"%s\"", k, v))
			}
		}
	}
	return sb.String()
}

/*
Create a request for the LLM (custom messages parameter)
*/
func (llm LLM) NewLLMRequest(tools []Tool, maxTokens int, system []Content, temperature float32, messages []Message) *LLMRequest {
	return &LLMRequest{
		Model:       llm.Model,
		MaxTokens:   maxTokens,
		Tools:       tools,
		Messages:    messages,
		System:      system,
		Temperature: temperature,
	}
}

/*
Create a request that takes in a user prompt
*/
// func (llm LLM) NewPromptRequest(maxTokens int, system []Content, temperature float32, prompt string) *LLMRequest {
// 	return &LLMRequest{
// 		Model:     llm.Model,
// 		MaxTokens: maxTokens,
// 		//Tools:     tools,
// 		Messages: []Message{
// 			{Role: "user", Content: prompt},
// 		},
// 		System:      system,
// 		Temperature: temperature,
// 	}
// }

/*
Create a request that involves a user prompt as well as a partially filled response
*/
// func (llm LLM) NewPromptRequestWithResponseStarter(tools []Tool, maxTokens int, system []Content, temperature float32, prompt string, responseStarter string) *LLMRequest {
// 	return &LLMRequest{
// 		Model:     llm.Model,
// 		MaxTokens: maxTokens,
// 		Messages: []Message{
// 			{Role: "user", Content: prompt},
// 			{Role: "assistant", Content: responseStarter},
// 		},
// 		System:      system,
// 		Temperature: temperature,
// 	}
// }

/*
Function for calling an LLM via a prompt.
Returns the Response, the status code of the request, and error, if applicable
*/
func (llm LLM) call(reqData LLMRequest) (*LLMResponse, int, error) {
	// Extracting the request data
	reqBody, err := json.Marshal(reqData)
	if err != nil {
		fmt.Printf("Error marshaling request data: %v\n", err)
		return nil, -1, err
	}

	// Creating the request
	url := "https://api.anthropic.com/v1/messages"
	req, err := http.NewRequest("POST", url, bytes.NewBuffer(reqBody))

	if err != nil {
		fmt.Printf("Error creating request:%v\n", err)
		return nil, -1, err
	}

	// Setting request headers
	fmt.Println("Setting request headers")
	req.Header.Set("x-api-key", llm.ApiKey)
	req.Header.Set("anthropic-version", "2023-06-01")
	req.Header.Set("content-type", "application/json")

	fmt.Println("Making request")

	// Sending the response
	client := &http.Client{}
	res, err := client.Do(req)
	if err != nil {
		fmt.Printf("Error sending request %v\n", err)
		return nil, res.StatusCode, err
	}

	// Closing the response body once the function is finished executing
	defer res.Body.Close()

	fmt.Println("Reading response data")
	// Reading the response data
	resBody, err := io.ReadAll(res.Body)
	if err != nil {
		fmt.Printf("Error reading response %v\n", err)
		return nil, res.StatusCode, err
	}

	if res.StatusCode != http.StatusOK {
		fmt.Printf("Request failed with status code %d: %s\n", res.StatusCode, string(resBody))
	}

	fmt.Println("Storing response")
	// Storing the response into the Response struct
	var apiRes LLMResponse
	err = json.Unmarshal(resBody, &apiRes)
	if err != nil {
		fmt.Printf("Error parsing the response: %v\n", err)
		return nil, res.StatusCode, err
	}

	return &apiRes, res.StatusCode, nil

}

/*
Takes the output from the LLM and adds it to the conversation history.
*/
func (llm *LLM) addResponseToChatHistory(response LLMResponse) {
	// Updating the chat history with the last output from the LLM

	// A message can contain multiple Content structs
	messageToAppend := Message{Role: "assistant", Content: []Content{}}
	messageToAppend.Content = append(messageToAppend.Content, response.Content...)

	llm.Messages = append(llm.Messages, messageToAppend)
}

// func getWeather(args ...interface{}) interface{} {
// 	if len(args) == 0 {
// 		return "Error: No location provided"
// 	}

// 	location, ok := args[0].(string)
// 	if !ok {
// 		return "Error: Invalid location type"
// 	}

// 	switch location {
// 	case "San Francisco, CA":
// 		return float32(24.0)
// 	case "Boston, MA":
// 		return float32(10.0)
// 	case "New York City, NY":
// 		return float32(8.0)
// 	default:
// 		return float32(5.0)
// 	}
// }
